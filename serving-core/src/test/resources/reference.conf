# API
plugin.version = 0.0.1
sparta.api.host = 0.0.0.0
sparta.api.port = 9090
sparta.api.certificate-file = "/home/user/certifications/stratio.jks"
sparta.api.certificate-password = "stratio"

# Api timeouts
sparta.api.timeout = 20
sparta.api.timeout = ${?SPARTA_TIMEOUT_API_CALLS}

# ZOOKEEPER

sparta.zookeeper.connectionString = "localhost:2181"
sparta.zookeeper.connectionTimeout = 15000
sparta.zookeeper.sessionTimeout = 60000
sparta.zookeeper.retryAttempts = 5
sparta.zookeeper.retryInterval = 10000


# OAUTH2

oauth2.enable = "false"
# oauth2.cookieName="user"
# oauth2.url.authorize = "https://server.domain:9005/cas/oauth2.0/authorize"
# oauth2.url.accessToken = "https://server.domain:9005/cas/oauth2.0/accessToken"
# oauth2.url.profile = "https://server.domain:9005/cas/oauth2.0/profile"
# oauth2.url.logout = "https://server.domain:9005/cas/logout"
# oauth2.url.callBack = "http://callback.domain:9090/login"
# oauth2.url.onLoginGoTo = "/"
# oauth2.client.id = "userid"
# oauth2.client.secret = "usersecret"


# SPRAY

spray.can.server.ssl-encryption = "off"
spray.can.server.request-timeout = 20s


# AKKA

akka.log-dead-letters = off
akka.loggers = ["akka.event.slf4j.Slf4jLogger"]
akka.logger-startup-timeout = 30s
akka.persistence.journal.plugin = "inmemory-journal"
akka.persistence.snapshot-store.plugin = "inmemory-snapshot-store"
akka.persistence.journal.leveldb.native = off


# CONFIG

# Driver jar served by Sparta in this location
sparta.config.driverPackageLocation = "/tmp/sparta/driver"

# Plugin jar are saved in this dir
sparta.config.pluginsLocation = "plugins"

# Backups files are saved in this dir
sparta.config.backupsLocation = "/tmp/sparta/backups"

# Maximun time when await to policy status change when run polices in cluster mode
sparta.config.awaitPolicyChangeStatus = 180s

# Api timeouts
sparta.config.timeout = 20
sparta.config.timeout = ${?SPARTA_TIMEOUT_API_CALLS}

sparta.config.version = ""


####################################
#                                  #
#      Security configuration      #
#                                  #
####################################

# Sparta security mode implementation based on the GoSec plugin
sparta.security.manager.enabled = false

# HDFS

# The hadoop user name could be configured by two ways:
# 1. Using the enviroment variable HADOOP_USER_NAME
# 2. Using the variable hadoopUserName from properties file
# sparta.hdfs.hadoopUserName = sparta

# If the variable HADOOP_CONF_DIR is not defined, "hdfsMaster" variable and "hdfsPort" are used to
# connect to HDFS cluster in order to upload jars to HDFS (plugins and driver), but the Spark executors and the
# Spark driver need this environment variable defined. In producction environments is recomended use
# HADOOP_CONF_DIR because use HA in Hadoop Namenodes, and omit "hdfsMaster and hdfsPort property"
# sparta.hdfs.hdfsMaster = hadoopNameNodeAddress
# sparta.hdfs.hdfsPort = 9000

# Configuration to connect to HDFS Kerberized

# The principal name could be configured by two ways:
# 1. Using the enviroment variable SPARTA_PRINCIPAL_NAME
# 2. Using the variable principalName from properties file
# The principal name used to connect to HDFS securized have the order 1, 2
# sparta.hdfs.principalName = ""

# The keytab path could be configured by two ways:
# 1. Using the enviroment variable SPARTA_KEYTAB_PATH
# 2. Using the variable keytabPath from properties file

# sparta.hdfs.keytabPath = ""
# sparta.hdfs.reloadKeyTab = false
# sparta.hdfs.reloadKeyTabTime = 23h


### SPARK MARATHON deployments ###

sparta.marathon.docker.image = "qa.stratio.com/stratio/sparta:2.0.0"
sparta.marathon.docker.forcePullImage = false
sparta.marathon.docker.privileged = false
sparta.marathon.jar = "/opt/sds/sparta/driver/sparta-driver.jar"
sparta.marathon.template.file = "/etc/sds/sparta/marathon-app-template.json"
sparta.marathon.mesosphere.lib = "/opt/mesosphere/lib"
sparta.marathon.mesosphere.packages = "/opt/mesosphere/packages"
sparta.marathon.gracePeriodSeconds = 180
sparta.marathon.intervalSeconds = 60
sparta.marathon.timeoutSeconds = 20
sparta.marathon.maxConsecutiveFailures = 3
# sparta.marathon.sso.uri = "https://gosec2.labs.stratio.com:9005/gosec-sso"
# sparta.marathon.sso.username = "admin"
# sparta.marathon.sso.password = "1234"
# sparta.marathon.sso.clientId = "adminrouter_paas-master-1.labs.stratio.com"
# sparta.marathon.sso.redirectUri = "https://master-1.labs.stratio.com/acs/api/v1/auth/login&firstUser=false"
# sparta.marathon.tikitakka.marathon.uri = "http://master-1.labs.stratio.com:8080"
sparta.marathon.tikitakka.marathon.api.version = "v2"


sparta.postgres.host = "localhost:5432"
sparta.postgres.host = ${?SPARTA_POSTGRES_HOST}
sparta.postgres.database = "postgres"
sparta.postgres.database = ${?SPARTA_POSTGRES_DATABASE}
sparta.postgres.extraParams = "prepareThreshold=0&leakDetectionThreshold=10000"
sparta.postgres.extraParams = ${?SPARTA_POSTGRES_EXTRAPARAMS}
sparta.postgres.schemaName = "spartatest"
sparta.postgres.schemaName = ${?SPARTA_POSTGRES_SCHEMA_NAME}
sparta.postgres.user = "postgres"
sparta.postgres.user = ${?SPARTA_POSTGRES_USER}
sparta.postgres.driver = org.postgresql.Driver
sparta.postgres.connectionPool = "HikariCP"
sparta.postgres.initializationFailFast = true
sparta.postgres.poolName = "spartaPostgresPool"
sparta.postgres.keepAliveConnection = true
sparta.postgres.keepAliveConnection = ${?SPARTA_POSTGRES_KEEP_ALIVE}
sparta.postgres.executionContext.parallelism = 5
sparta.postgres.executionContext.parallelism = ${?SPARTA_POSTGRES_EXECUTION_CONTEXT_PARALLELISM}
sparta.postgres.sslenabled = false
sparta.postgres.sslenabled = ${?SECURITY_TLS_ENABLE}
sparta.postgres.sslcert = ${?SPARTA_TLS_CERT}
sparta.postgres.sslkey = ${?SPARTA_TLS_KEY_PKCS8}
sparta.postgres.sslrootcert = ${?SPARTA_TLS_ROOTCERT}

# HikariCP
sparta.postgres.connectionPool = "HikariCP"
sparta.postgres.initializationFailFast = true
sparta.postgres.poolName =  "spartaPostgresPool"
sparta.postgres.numThreads = 1
sparta.postgres.numThreads = ${?SPARTA_POSTGRES_POOL_THREADS}
sparta.postgres.queueSize = 3000
sparta.postgres.queueSize = ${?SPARTA_POSTGRES_POOL_QUEUE_SIZE}
sparta.postgres.maxConnections = 1
sparta.postgres.maxConnections = ${?SPARTA_POSTGRES_POOL_MAX_CONNECTIONS}
sparta.postgres.minConnections = 1
sparta.postgres.minConnections = ${?SPARTA_POSTGRES_POOL_MIN_CONECTIONS}

# HDFS
# The hadoop user name could be configured by two ways:
# 1. Using the enviroment variable HADOOP_USER_NAME
# 2. Using the variable hadoopUserName from properties file
sparta.hdfs.hadoopUserName = ${?SPARTA_HADOOP_USER_NAME}
# If the variable HADOOP_CONF_DIR is not defined, "hdfsMaster" variable and "hdfsPort" are used to
# connect to HDFS cluster in order to upload jars to HDFS, but the Spark executors and the
# Spark driver need this environment variable defined. In producction environments is recomended use
# HADOOP_CONF_DIR because use HA in Hadoop Namenodes
sparta.hdfs.hdfsMaster = ${?HADOOP_FS_DEFAULT_NAME}
sparta.hdfs.hdfsPort = ${?HADOOP_PORT}
# Configuration to connect to HDFS Kerberized
# The principal name could be configured by two ways:
# 1. Using the enviroment variable SPARTA_PRINCIPAL_NAME
# 2. Using the variable principalName from properties file
# The principal name used to connect to HDFS securized have the order 1, 2
sparta.hdfs.principalName = ${?SPARTA_PRINCIPAL_NAME}
# The keytab path could be configured by two ways:
# 1. Using the enviroment variable SPARTA_KEYTAB_PATH
# 2. Using the variable keytabPath from properties file
sparta.hdfs.keytabPath = ${?SPARTA_KEYTAB_PATH}
sparta.hdfs.reloadKeyTab = ${?HDFS_KEYTAB_RELOAD}
sparta.hdfs.reloadKeyTabTime = ${?HDFS_KEYTAB_RELOAD_TIME}

# SPARK
spark.master = "local[2]"
spark.master = ${?CROSSDATA_SERVER_CONFIG_SPARK_MASTER}
spark.ui.enabled = ${?CROSSDATA_SERVER_SPARK_UI_ENABLED}
spark.ui.port = 4041
spark.ui.port = ${?CROSSDATA_SERVER_CONFIG_SPARK_UI_PORT}
spark.executor.memory = ${?CROSSDATA_SERVER_CONFIG_SPARK_EXECUTOR_MEMORY}
spark.executor.cores = ${?CROSSDATA_SERVER_CONFIG_SPARK_EXECUTOR_CORES}
spark.driver.memory = ${?CROSSDATA_SERVER_CONFIG_SPARK_DRIVER_MEMORY}
spark.driver.cores = ${?CROSSDATA_SERVER_CONFIG_SPARK_DRIVER_CORES}
spark.cores.max = 1
spark.cores.max = ${?CROSSDATA_SERVER_CONFIG_SPARK_CORES_MAX}
spark.locality.wait = 100
spark.locality.wait = ${?CROSSDATA_SERVER_CONFIG_SPARK_LOCALITY_WAIT}
spark.mesos.coarse = true
spark.sql.caseSensitive = true
spark.sql.caseSensitive = ${?CROSSDATA_SERVER_CONFIG_SPARK_SQL_CASESENSITIVE}
spark.mesos.executor.home = "/opt/spark/dist"
spark.mesos.executor.docker.image = "qa.stratio.com/stratio/spark-stratio-driver:2.2.0-2.1.0-f969ad8"
spark.mesos.executor.docker.image = ${?CROSSDATA_SERVER_CONFIG_SPARK_DOCKER_IMAGE}
spark.mesos.executor.docker.volumes = "/opt/mesosphere/packages/:/opt/mesosphere/packages/:ro,/opt/mesosphere/lib/:/opt/mesosphere/lib/:ro,/etc/pki/ca-trust/extracted/java/cacerts/:/usr/lib/jvm/jre1.8.0_112/lib/security/cacerts:ro,/etc/resolv.conf:/etc/resolv.conf:ro"
spark.mesos.executor.docker.volumes = ${?CROSSDATA_SERVER_CONFIG_SPARK_DOCKER_VOLUMES}
spark.hadoop.fs.hdfs.impl.disable.cache = true
spark.hadoop.fs.hdfs.impl.disable.cache = ${?CROSSDATA_HDFS_DELEGATION_TOKEN_DISABLE_CACHE}
spark.mesos.role = ${?SPARK_MESOS_ROLE}
spark.mesos.principal = ${?SPARK_MESOS_PRINCIPAL}
spark.mesos.secret = ${?SPARK_MESOS_SECRET}
spark.mesos.executor.docker.network.name = ${?CALICO_NETWORK}
spark.executorEnv.SPARK_LOG_LEVEL = "ERROR"
spark.executorEnv.SPARK_LOG_LEVEL = ${?SPARK_LOG_LEVEL}
spark.executorEnv.MESOS_NATIVE_JAVA_LIBRARY = "/opt/mesosphere/lib/libmesos.so"
spark.executorEnv.VAULT_HOSTS = ${?VAULT_HOSTS}
spark.executorEnv.VAULT_PROTOCOL = ${?VAULT_PROTOCOL}
spark.executorEnv.VAULT_PORT = ${?VAULT_PORT}
spark.executorEnv.SPARK_DRIVER_SECRET_FOLDER = ${?SPARK_DRIVER_SECRET_FOLDER}
spark.executorEnv.SPARK_SECURITY_DATASTORE_ENABLE = ${?SPARK_SECURITY_DATASTORE_ENABLE}
spark.executorEnv.SPARK_SECURITY_DATASTORE_VAULT_TRUSTSTORE_PATH = ${?SPARK_SECURITY_DATASTORE_VAULT_TRUSTSTORE_PATH}
spark.executorEnv.SPARK_SECURITY_DATASTORE_VAULT_TRUSTSTORE_PASS_PATH = ${?SPARK_SECURITY_DATASTORE_VAULT_TRUSTSTORE_PASS_PATH}
spark.executorEnv.SPARK_SECURITY_DATASTORE_VAULT_CERT_PATH = ${?SPARK_SECURITY_DATASTORE_VAULT_CERT_PATH}
spark.executorEnv.SPARK_SECURITY_DATASTORE_VAULT_CERT_PASS_PATH = ${?SPARK_SECURITY_DATASTORE_VAULT_CERT_PASS_PATH}
spark.executorEnv.SPARK_SECURITY_DATASTORE_VAULT_KEY_PASS_PATH = ${?SPARK_SECURITY_DATASTORE_VAULT_KEY_PASS_PATH}