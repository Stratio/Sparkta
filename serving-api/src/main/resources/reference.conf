sparta {

  api {
    host = 0.0.0.0
    port = 9090
    certificate-file = "/home/user/certifications/stratio.jks"
    certificate-password = "stratio"
  }

  swagger {
    host = 0.0.0.0
    port = 9091
  }

  zookeeper {
    connectionString = "localhost:2181"
    connectionTimeout = 15000
    sessionTimeout = 60000
    retryAttempts = 5
    retryInterval = 10000
  }

  akka {
    controllerActorInstances = 5
  }

  config {

    # The execution modes in Sparta are: local, mesos, yarn and standAlone
    executionMode = local

    # The rememberPartitioner for RDD generated in Stateful operations in Spark, like UpdateStateByKey
    rememberPartitioner = true

    # In cluster mode and production environment is recommended set to true
    stopGracefully = true

    # Maximun time when await time to spark jobs termination before force the spark context termination and
    # application exit
    awaitStreamingContextStop = 120s
    awaitSparkContextStop = 60s

    # Maximun time when await to policy status change
    awaitPolicyChangeStatus = 120s

    # All the checkpoint options is possible to set it in the policy

    # Example value in cluster mode
    # checkpointPath = "/user/stratio/sparta/checkpoint"

    # Example value in local mode for Stratio Platform
    # checkpointPath = "/var/sds/sparta/checkpoint"

    # Example value in local mode for debugging
    checkpointPath = "/tmp/sparta/checkpoint"

    # Auto delete checkpoint when run policies
    autoDeleteCheckpoint = true
  }

  hdfs {

    # The hadoop user name could be configured by two ways:
    #   1. Using the enviroment variable HADOOP_USER_NAME
    #   2. Using the variable hadoopUserName from properties file
    hadoopUserName = root

    # If the variable HADOOP_CONF_DIR is not defined, "hdfsMaster" variable and "hdfsPort" are used to
    # connect to HDFS cluster in order to upload jars to HDFS (plugins and driver), but the Spark executors and the
    # Spark driver need this environment variable defined. In producction environments is recomended use
    # HADOOP_CONF_DIR because use HA in Hadoop Namenodes, and omit "hdfsMaster and hdfsPort property"
    hdfsMaster = hadoopNameNodeAddress
    hdfsPort = 9000

    # Folders created in HDFS when run over Mesos, Yarn or StandAlone clusters
    pluginsFolder = plugins
    executionJarFolder = jarDriver
    classpathFolder = classpath

    # Configuration to connect to HDFS Kerberized

    # The principal name could be configured by three ways:
    #   1. Using the enviroment variable HADOOP_PRINCIPAL_NAME
    #   2. Using the variables "principalNamePrefix" and "principalNameSuffix" and the enviroment variable HOSTNAME
    #   3. Using the variable principalName from properties file
    # The principal name used to connect to HDFS securized have the order 1, 2 and finally 3.

    # principalName = ""
    # principalNamePrefix = ""
    # principalNameSuffix = ""


    # The keytab path could be configured by two ways:
    #   1. Using the enviroment variable HADOOP_KEYTAB_PATH
    #   2. Using the variable keytabPath from properties file

    # keytabPath = ""
    # reloadKeyTabTime = 23m
  }

  # Spark Options for development environments
  local {

    spark.app.name = SPARTA
    spark.master = "local[*]"
    spark.driver.memory = 1G
    spark.driver.cores = 1
    spark.executor.memory = 1G
    spark.app.name = SPARTA

    # Running concurrent jobs brings down the overall processing time and scheduling delay even if a batch
    # takes processing time slightly more than batch interval.
    # By default the number of concurrent jobs is 1 which means at a time only 1 job will be active
    # and till its not finished,other jobs will be queued up even if the resources are available and idle.
    # spark.streaming.concurrentJobs = 1

    # Turn this down to prevent long blocking at shutdown
    # spark.streaming.gracefulStopTimeout = 100000

    # Option necessary when run benchmarks
    # spark.metrics.conf = /opt/sds/sparta/benchmark/src/main/resources/metrics.properties

    # Use other serializer than default Java Serializer
    # spark.serializer = org.apache.spark.serializer.KryoSerializer

    # Parquet prevention errors
    spark.sql.parquet.binaryAsString = true
  }

  # Spark Options for Mesos environments
  mesos {

    # If the environment variable SPARK_HOME is defined, the spark submit use the variable
    sparkHome = ""

    # Application Name, is possible to assign with "name" and the property spark.app.name.
    # spark.app.name = SPARTA

    # Coarse mode is recommended in Streaming Applications over Mesos Clusters. If is assinged false is used the
    # Spark fine grained mode
    spark.mesos.coarse = true

    # Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
    # variable spark.submit.deployMode
    deployMode = cluster

    # In Cluster mode is the SparkMesosDispatcher URI, in Client mode the Mesos Master URI
    master = "mesos://mesosDispatcherURI"

    # Comma-separated list of local jars to include on the driver and executor classpaths
    # jars = ""

    # Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
    # the local maven repo, then maven central and any additional remote repositories given by --repositories. The
    # format for the coordinates should be groupId:artifactId:version
    # packages = "com:stratio:package"

    # Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
    # to avoid dependency conflicts
    # exclude-packages = "com:stratio:package"

    # Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
    # repositories = "hdfs://repo"

    # User to impersonate when submitting the application. This argument does not work with --principal / --keytab
    # proxy-user = username

    # Extra Java options to pass to the driver
    # driver-java-options = ""

    # Extra library path entries to pass to the driver
    # driver-library-path = ""

    # Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
    # the classpath
    # driver-class-path = ""

    # Memory by executor
    spark.executor.memory = 1G

    # Total cores by executors, is possible to assign the varibale spark.cores.max.
    # If you donâ€™t set spark.cores.max, the Spark application will reserve all resources offered to it by Mesos
    totalExecutorCores = 2

      # IMPORTANT in SPARK 2.x!! Number of executors in Mesos: spark.cores.max/spark.executor.cores
      # Cores by executor only in SPARK 2.x
      # spark.executor.cores = 1

      # (Fine-grained mode only) Number of cores to give each Mesos executor
      # spark.mesos.mesosExecutor.cores = 1

    # Extra amount of cpus to request per task
    # spark.mesos.extra.cores = 0

    # Cores assigned to Spark Driver. Cluster mode only
    spark.driver.cores = 1

    # Memory assigned to Spark Driver.
    spark.driver.memory = 1G

    # Restarts the driver on failure, is the same as the property spark.driver.supervise. Cluster mode only
    supervise = false

    # Running concurrent jobs brings down the overall processing time and scheduling delay even if a batch
    # takes processing time slightly more than batch interval.
    # By default the number of concurrent jobs is 1 which means at a time only 1 job will be active
    # and till its not finished,other jobs will be queued up even if the resources are available and idle.
    # spark.streaming.concurrentJobs = 1

    # Turn this down to prevent long blocking at shutdown
    # spark.streaming.gracefulStopTimeout = 100000

    # Use other serializer than default Java Serializer
    # spark.serializer = org.apache.spark.serializer.KryoSerializer

    # Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
    # propertiesFile = ""

    # Run over Mesos Clusters with Docker conatiners
    # spark.mesos.executor.docker.image=dockerAccount/spark-1.6.2-bin-2.6.0

    # If Spark in Mesos Slaves are installed in a different path than spark submit installation
    # spark.executor.home=/spark-1.6.2-bin-2.6.0
    # spark.executor.uri=/spark-mesosphere-scala211-1.6.2-bin-hadoop2.6.0.tgz

    # Available in all execution modes
    # spark.driver.extraClassPath = ""
    # spark.driver.extraJavaOptions = ""
    # spark.driver.extraLibraryPath = ""
    # spark.jars = ""
    # spark.files = ""

    # Available in client mode
    # spark.jars.ivy = ""

    # Parquet prevention errors
    spark.sql.parquet.binaryAsString = true
  }

  # Spark Options for Yarn environments
  yarn {

    # If the environment variable SPARK_HOME is defined, the spark submit use the variable
    sparkHome = ""

    # Yarn cluster name
    master = yarn-cluster

    # Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
    # variable spark.submit.deployMode
    deployMode = cluster

    # Comma-separated list of local jars to include on the driver and executor classpaths
    # jars = ""

    # Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
    # the local maven repo, then maven central and any additional remote repositories given by --repositories. The
    # format for the coordinates should be groupId:artifactId:version
    # packages = "com:stratio:package"

    # Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
    # to avoid dependency conflicts
    # exclude-packages = "com:stratio:package"

    # Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
    # repositories = "hdfs://repo"

    # User to impersonate when submitting the application. This argument does not work with --principal / --keytab
    # proxy-user = username

    # Extra Java options to pass to the driver
    # driver-java-options = ""

    # Extra library path entries to pass to the driver
    # driver-library-path = ""

    # Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
    # the classpath
    # driver-class-path = ""

    # Application Name, is possible to assign with "name" and the property spark.app.name.
    # name = SPARTA

    # Number of executors, is the same as the property spark.executor.instances.
    numExecutors = 1

    # Memory by executor, is the same as the property spark.executor.memory.
    executorMemory = 1G

    # Cores by executor, is possible to assign the varibale spark.executor.cores.
    executorCores = 1

    # Memory assigned to Spark Driver. Cluster mode only.
    driverMemory = 1G

    # Memory assigned to Spark Driver. Client mode only.
    # spark.driver.memory = 1G

    # Cores assigned to Spark Driver, is the same as the property spark.driver.cores. Cluster mode only.
    driverCores = 1

    # The YARN queue to submit to (Default: "default"). Client mode only.
    # spark.yarn.queue = queuename

    # Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
    # propertiesFile = ""

    # Comma separated list of archives to be extracted into the working directory of each executor. Client mode only.
    # spark.yarn.dist.files = ""
    # spark.yarn.dist.archives = ""

    # Comma separated list of archives to be extracted into the working directory of each executor. Cluster mode only.
    # files = ""
    # archives = ""

    # Comma-separated list of jars
    # addJars = ""

    # Available in all execution modes
    # spark.driver.extraClassPath = ""
    # spark.driver.extraJavaOptions = ""
    # spark.driver.extraLibraryPath = ""

    # Available in client mode
    # spark.jars = ""
    # spark.jars.ivy = ""

    # Parquet prevention errors
    spark.sql.parquet.binaryAsString = true
  }

  # Spark Options for Spark StandAlone environments
  standalone {

    # If the environment variable SPARK_HOME is defined, the spark submit use the variable
    sparkHome = ""

    # Spark Master URI
    master = "spark://127.0.0.1:7077"

    # Cluster or Client. If the user need more than one policy running is necessary use "cluster". Is the same as the
    # variable spark.submit.deployMode
    deployMode = cluster

    # Comma-separated list of local jars to include on the driver and executor classpaths
    # jars = ""

    # Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search
    # the local maven repo, then maven central and any additional remote repositories given by --repositories. The
    # format for the coordinates should be groupId:artifactId:version
    # packages = "com:stratio:package"

    # Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages
    # to avoid dependency conflicts
    # exclude-packages = "com:stratio:package"

    # Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages
    # repositories = "hdfs://repo"

    # User to impersonate when submitting the application. This argument does not work with --principal / --keytab
    # proxy-user = username

    # Extra Java options to pass to the driver
    # driver-java-options = ""

    # Extra library path entries to pass to the driver
    # driver-library-path = ""

    # Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in
    # the classpath
    # driver-class-path = ""

    # Application Name, is possible to assign with "name" and the property spark.app.name.
    # spark.app.name = SPARTA

    # Cores assigned to Spark Driver. Cluster mode only
    spark.driver.cores = 1

    # Memory assigned to Spark Driver.
    spark.driver.memory = 1G

    # Cores by executor
    spark.executor.cores = 1

    # Memory by executor
    spark.executor.memory = 1G

    # Total cores by executors, is possible to assign the varibale spark.cores.max
    totalExecutorCores = 2

    # Restarts the driver on failure, is the same as the property spark.driver.supervise. Cluster mode only
    supervise = false

    # Path to a file from which to load extra properties. If notspecified, this will look for conf/spark-defaults.conf.
    # propertiesFile = ""

    # Available in all execution modes
    # spark.driver.extraClassPath = ""
    # spark.driver.extraJavaOptions = ""
    # spark.driver.extraLibraryPath = ""
    # spark.jars = ""
    # spark.jars.ivy = ""
    # spark.files = ""

    # Parquet prevention errors
    spark.sql.parquet.binaryAsString = true
  }
}

oauth2 {
  enable = "false"

  url {
    authorize = "https://server.domain:9005/cas/oauth2.0/authorize"
    accessToken = "https://server.domain:9005/cas/oauth2.0/accessToken"
    profile = "https://server.domain:9005/cas/oauth2.0/profile"
    logout = "https://server.domain:9005/cas/logout"
    callBack = "http://callback.domain:9090/login"
    onLoginGoTo = "/"
  }
  client{
    id = "userid"
    secret = "usersecret"
  }
  cookieName="user"
}

spray.can {
  server {
    ssl-encryption = "off"
  }
}

akka {
  log-dead-letters = off
}