{
  "basicSettings": [{
      "propertyId": "name",
      "propertyName": "SETTINGS._NAME_",
      "propertyType": "text",
      "regexp": "^[a-z0-9-]*$",
      "required": true,
      "qa": "workflow-name",
      "newRow": true,
      "col": 5
    },
    {
      "propertyId": "description",
      "propertyName": "SETTINGS._DESCRIPTION_",
      "propertyType": "text",
      "required": false,
      "newRow": true,
      "qa": "workflow-description",
      "col": 7
    }
  ],
  "advancedSettings": [
    {
      "name" : "global",
      "listCompressed": true,
      "properties": [
        {
          "propertyId": "executionMode",
          "propertyName": "SETTINGS._EXECUTION_MODE_",
          "required": true,
          "propertyType": "select",
          "width": 4,
          "default": "marathon",
          "values": [{
            "label": "Local",
            "value": "local"
          },
            {
              "label": "Mesos",
              "value": "mesos"
            },
            {
              "label": "Marathon",
              "value": "marathon"
            }
          ],
          "tooltip": "Execution mode used to launch the existing workflows, if nothing is selected Sparta will use the values in the configuration file",
          "qa": "workflow-execution-mode"
        },
        {
          "propertyId": "initSqlSentences",
          "propertyName": "SETTINGS._SQL_SENTENCES_",
          "propertyType": "list",
          "tooltip": "SQL sentences that will be executed when the workflow starts. It's useful when the user needs to registry temporal/master tables from external datasources. Once launched it's possible to mix them inside the triggers",
          "qa": "workflow-sql-sentences",
          "fields": [{
            "propertyId": "sentence",
            "propertyName": "SETTINGS._SQL_SENTENCE_",
            "propertyType": "text",
            "regexp": "",
            "default": "",
            "required": false,
            "tooltip": "SQL sentence",
            "qa": "workflow-sql-sentence"
          }]
        },
         {
            "propertyId": "userPluginsJars",
            "propertyName": "SETTINGS._USER_PLUGINS_JARS_",
            "propertyType": "list",
            "tooltip": "Custom jars with plugins created by the user that will be used in the Spark Jobs. It's possible to add library jars or Sparta plugins that extend the Sparta SDK",
            "qa": "workflow-user-plugins-jars",
            "fields": [{
               "propertyId": "jarPath",
               "propertyName": "SETTINGS._JAR_PATH",
               "propertyType": "text",
               "regexp": "",
               "default": "",
               "required": false,
               "qa": "workflow-jar-path"
            }]
         },
         {
            "propertyId": "addAllUploadedPlugins",
            "propertyName": "SETTINGS._ADD_ALL_UPLOADED_PLUGINS_",
            "propertyType": "boolean",
            "required": true,
            "default": true,
            "width": "auto",
            "tooltip": "Enables or disables all the uploaded plugins for this workflow",
            "qa": "spark-streaming-addplugins"
         }
      ]
    },
    {
      "name" : "streamingSettings",
      "properties": [
        {
          "propertyId": "window",
          "propertyName": "SETTINGS._SPARK_STREAMING_WINDOW_",
          "propertyType": "text",
          "default": "2s",
          "tooltip": "Streaming window assigned to Spark",
          "required": true,
           "width": 4,
          "qa": "spark-streaming-window"
        },
        {
          "propertyId": "remember",
          "propertyName": "SETTINGS._WORKFLOW_REMEMBER_FIELD_",
          "propertyType": "text",
          "tooltip": "Maximum time a certain query will be executed before being aborted",
          "required": false,
           "width": 4,
          "qa": "spark-streaming-remember-number"
        },
        {
          "propertyId": "backpressure",
          "propertyName": "SETTINGS._SPARK_STREAMING_BACK_PRESSURE_",
          "propertyType": "switch",
          "required": true,
          "default": false,
           "width": "auto",
          "tooltip": "Enables or disables Spark Streaming's internal backpressure mechanism",
          "qa": "spark-streaming-backpressure"
        },
        {
          "propertyId": "backpressureInitialRate",
          "propertyName": "SETTINGS._SPARK_STREAMING_BACK_PRESSURE_INITIAL_RATE_",
          "propertyType": "text",
          "tooltip": "Maximum time a certain query will be executed before being aborted",
          "required": false,
           "visible": [
              [
                 {
                    "propertyId": "backpressure",
                    "value": true
                 }
              ]
           ],
           "width": 4,
          "qa": "spark-streaming-backpressure-initial-rate"
        },
        {
          "name" : "checkpointSettings",
          "properties": [
             {
                "propertyId": "enableCheckpointing",
                "propertyName": "SETTINGS._ENABLE_CHECKPOINT_",
                "propertyType": "switch",
                "width": "auto",
                "required": true,
                "default": true,
                "tooltip": "Enable checkpointing in the streaming process, the Spark job will save the RDDs in the specified path in HDFS. It is mandatory when the workflow has cubes",
                "qa": "workflow-enable-checkpoint"
             },
               {
                  "propertyId": "checkpointPath",
                  "propertyName": "SETTINGS._CHECKPOINT_PATH_",
                  "propertyType": "text",
                  "default": "sparta/checkpoint",
                  "required": false,
                  "tooltip": "Used only if the checkpointing feature is activated. When selecting Mesos or Marathon as deploy mode the checkpoint path must be one located in a HDFS node(sparta/checkpoint), when deploying in Local the checkpoint path indicates a local path in the machine (/tmp/sparta/checkpoint)",
                  "visible": [
                     [
                        {
                           "propertyId": "enableCheckpointing",
                           "value": true
                        }
                     ]
                  ],
                  "qa": "workflow-checkpoint-path"
               },

               {
                  "propertyId": "autoDeleteCheckpoint",
                  "propertyName": "SETTINGS._AUTO_DELETE_CHECKPOINT_",
                  "propertyType": "boolean",
                  "width": "auto",
                  "required": true,
                  "default": true,
                  "float": true,
                  "tooltip": "Only used when the checkpointing feature is enabled. Auto delete checkpoint path in HDFS when running policies",
                  "visible": [
                     [
                        {
                           "propertyId": "enableCheckpointing",
                           "value": true
                        }
                     ]
                  ],
                  "qa": "workflow-delete-checkpoint"
               },
               {
                  "propertyId": "addTimeToCheckpointPath",
                  "propertyName": "SETTINGS._ADD_TIME_TO_CHECKPOINT_",
                  "width": "auto",
                  "propertyType": "boolean",
                  "required": true,
                  "default": false,
                  "tooltip": "Only used when the checkpointing feature is enabled. Add the current time to checkpoint path. Be careful if you need HA in your streaming process",
                  "visible": [
                     [
                        {
                           "propertyId": "enableCheckpointing",
                           "value": true
                        }
                     ]
                  ],
                  "qa": "workflow-add-timestamp-checkpoint"
               }
          ]
        }
      ]
    },
    {
      "name" : "sparkSettings",
      "properties": [
        {
          "propertyId": "master",
          "width": 4,
          "propertyName": "SETTINGS._SPARK_MASTER_",
          "propertyType": "text",
          "default": "mesos://leader.mesos:5050",
          "toltip": "Sets the spark master. In Marathon deployments: mesos://leader.mesos:5050 . Local deployments:local[*] . Dispatcher deployments: dispatcherURI:7077",
          "qa": "workflow-spark-user"
        },
        {
          "propertyId": "sparkKerberos",
          "propertyName": "SETTINGS._SPARK_KERBEROS_ENABLE_",
          "propertyType": "boolean",
          "width": "auto",
          "default": false,
          "float": true,
          "tooltip": "Enables Kerberos integration and send the principal and keytab arguments if  available through the spark submit",
          "qa": "workflow-spark-kerberos"
        },
        {
          "propertyId": "sparkDataStoreTls",
          "propertyName": "SETTINGS._SPARK_DATASTORE_TLS_ENABLE_",
          "propertyType": "boolean",
          "width": "auto",
          "default": false,
          "tooltip": "Enables dataStore with TLS integration and send options through the spark submit",
          "qa": "workflow-spark-datastore-tls"
        },
        {
           "propertyId": "sparkMesosSecurity",
           "propertyName": "SETTINGS._SPARK_MESOS_SECURITY_ENABLE_",
           "propertyType": "boolean",
           "width": "auto",
           "default": false,
           "tooltip": "Enables MESOS security and send options through the spark submit",
           "qa": "workflow-spark-enable-mesos"
        },
        {
           "name": "submitArguments",
           "listCompressed": true,
            "properties": [
               {
                  "propertyId": "userArguments",
                  "propertyName": "SETTINGS._SPARK_SUBMIT_ARGUMENTS_",
                  "propertyType": "list",
                  "required": false,
                  "tooltip": "Spark submit arguments added when the workflow is executed on a Spark cluster. Values set here will override the property file options",
                  "qa": "workflow-spark-submit-argument",
                  "fields": [
                     {
                       "propertyId": "submitArgument",
                       "propertyName": "SETTINGS._SPARK_SUBMIT_ARGUMENT_KEY_",
                       "position": "left",
                       "propertyType": "text",
                       "regexp": "",
                       "default": "",
                        "width": 4,
                       "required": false,
                       "tooltip": "Spark submit argument with the structure --xxx",
                       "qa": "workflow-spark-submit-argument-key"
                     },
                     {
                       "propertyId": "submitValue",
                       "propertyName": "SETTINGS._SPARK_SUBMIT_ARGUMENT_VALUE_",
                       "position": "right",
                       "propertyType": "text",
                       "regexp": "",
                       "default": "",
                        "width": 4,
                       "required": false,
                       "qa": "workflow-spark-submit-argument-value"
                     }
                  ]
               },
               {
                  "propertyId": "deployMode",
                  "propertyName": "SETTINGS._SPARK_DEPLOY_MODE_",
                  "propertyType": "select",
                  "width": 4,
                  "values": [
                     {
                        "label": "Client",
                        "value": "client"
                     },
                     {
                        "label": "Cluster",
                        "value": "cluster"
                     }
                  ],
                  "default": "client",
                  "tooltip": "Sets the spark deploy mode in cluster executions",
                  "qa": "workflow-spark-deploy-mode"
               },
               {
                  "propertyId": "driverJavaOptions",
                  "propertyName": "SETTINGS._SPARK_DRIVER_JAVA_OPTIONS_",
                  "propertyType": "text",
                  "width": 4,
                  "float": true,
                  "default": "-XX:+UseConcMarkSweepGC",
                  "tooltip": "Extra Java options to pass to the driver",
                  "qa": "workflow-spark-driverJavaOptions"
               },
               {
                  "propertyId": "driverLibraryPath",
                  "propertyName": "SETTINGS._SPARK_DRIVER_LIBRARY_PATH_",
                  "propertyType": "text",
                  "width": 4,
                  "float": true,
                  "tooltip": "Extra library path entries to be passed to the driver",
                  "qa": "workflow-spark-driverLibraryPath"
               },
               {
                  "propertyId": "driverClassPath",
                  "propertyName": "SETTINGS._SPARK_DRIVER_CLASS_PATH_",
                  "propertyType": "text",
                  "width": 4,
                  "tooltip": "Extra class path entries to passed to the driver. Note that jars added with --jars are automatically included in the classpath",
                  "qa": "workflow-spark-driverClassPath"
               },
               {
                  "propertyId": "packages",
                  "propertyName": "SETTINGS._SPARK_PACKAGES_",
                  "propertyType": "text",
                  "width": 4,
                  "float": true,
                  "tooltip": "Comma-separated list of jars expressed as maven coordinates to include in the driver and executor classPaths. This will search first in the local maven repo, then in maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version",
                  "qa": "workflow-spark-packages"
               },
               {
                  "propertyId": "excludePackages",
                  "propertyName": "SETTINGS._SPARK_EXCLUDE_PACKAGES_",
                  "propertyType": "text",
                  "width": 4,
                  "float": true,
                  "tooltip": "Comma-separated list of groupId:artifactId coordinates to exclude while resolving the dependencies provided in --packages in order to avoid dependency conflicts",
                  "qa": "workflow-spark-excludePackages"
               },
               {
                  "propertyId": "repositories",
                  "propertyName": "SETTINGS._SPARK_REPOSITORIES_",
                  "propertyType": "text",
                  "width": 4,
                  "tooltip": "Comma-separated list of additional remote repositories where to look for the maven coordinates given with --packages",
                  "qa": "workflow-spark-repositories"
               },
               {
                  "propertyId": "jars",
                  "propertyName": "SETTINGS._SPARK_JARS_",
                  "propertyType": "text",
                  "width": 4,
                  "float": true,
                  "tooltip": "Comma-separated list of local jars to include on the driver and executor classPaths",
                  "qa": "workflow-spark-jars"
               },
               {
                  "propertyId": "propertiesFile",
                  "propertyName": "SETTINGS._SPARK_PROPERTIES_FILE_",
                  "propertyType": "text",
                  "width": 4,
                  "tooltip": "Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf",
                  "qa": "workflow-spark-propertiesFile"
               }
            ]
         },
        {
          "name": "sparkConf",
          "listCompressed": true,
          "properties": [
            {
                "name": "sparkResourcesConf",
                "listCompressed": true,
                "properties": [
                   {
                      "propertyId": "coresMax",
                      "propertyName": "SETTINGS._SPARK_CORES_MAX_",
                      "propertyType": "text",
                      "default": "2",
                      "width": 4,
                      "float": true,
                      "tooltip": "Maximum amount of CPU cores to request for the application",
                      "qa": "workflow-spark-coresMax"
                   },
                   {
                      "propertyId": "mesosExtraCores",
                      "propertyName": "SETTINGS._SPARK_EXTRA_CORES_",
                      "propertyType": "text",
                      "width": 4,
                      "tooltip": "Extra amount of CPUs to request per task",
                      "qa": "workflow-spark-mesosExtraCores"
                   },
                   {
                      "propertyId": "driverCores",
                      "propertyName": "SETTINGS._SPARK_DRIVER_CORES_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "1",
                      "float": true,
                      "tooltip": "Number of cores assigned to the Spark Driver",
                      "qa": "workflow-spark-driverCores"
                   },
                   {
                      "propertyId": "driverMemory",
                      "propertyName": "SETTINGS._SPARK_DRIVER_MEMORY_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "2G",
                      "tooltip": "Memory assigned to the Spark Driver",
                      "qa": "workflow-spark-driverMemory"
                   },
                   {
                      "propertyId": "executorCores",
                      "propertyName": "SETTINGS._SPARK_EXECUTOR_CORES_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "1",
                      "float": true,
                      "tooltip": "Number of cores to use on each executor",
                      "qa": "workflow-spark-executorCores"
                   },
                   {
                      "propertyId": "executorMemory",
                      "propertyName": "SETTINGS._SPARK_EXECUTOR_MEMORY_",
                      "propertyType": "text",
                      "default": "2G",
                      "width": 4,
                      "tooltip": "Amount of memory to use per executor process",
                      "qa": "workflow-spark-executorMemory"
                   },
                   {
                      "propertyId": "localityWait",
                      "propertyName": "SETTINGS._SPARK_LOCALITY_WAIT_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "100",
                      "tooltip": "Period of time to wait to launch a data-local task before giving up and launching it on a less-local node",
                      "qa": "workflow-spark-localityWait"
                   },
                   {
                      "propertyId": "taskMaxFailures",
                      "propertyName": "SETTINGS._SPARK_TASK_MAX_FAILURES_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "8",
                      "tooltip": "Number of failures of any particular task before giving up on the job",
                      "qa": "workflow-spark-taskMaxFailures"
                   },
                   {
                      "propertyId": "sparkMemoryFraction",
                      "propertyName": "SETTINGS._SPARK_MEMORY_FRACTION_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "0.6",
                      "tooltip": "Fraction of (heap space - 300MB) used for execution and storage",
                      "qa": "workflow-spark-memory-fraction"
                   }

                ]
             },
            {
                "name": "sparkDockerConf",
                "listCompressed": true,
                "properties": [
                   {
                      "propertyId": "executorDockerImage",
                      "propertyName": "SETTINGS._SPARK_DOCKER_IMAGE_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "qa.stratio.com/stratio/stratio-spark:2.1.0.4",
                      "tooltip": "Run over Mesos Clusters with Docker containers",
                      "qa": "workflow-spark-executorDockerImage"
                   },
                   {
                      "propertyId": "executorDockerVolumes",
                      "propertyName": "SETTINGS._SPARK_DOCKER_VOLUMES_",
                      "propertyType": "text",
                      "width": 4,
                      "default": "/opt/mesosphere/packages/:/opt/mesosphere/packages/:ro,/opt/mesosphere/lib/:/opt/mesosphere/lib/:ro",
                      "tooltip": "Mount volumes from host machine in the executor docker container",
                      "qa": "workflow-spark-executorDockerVolumes"
                   },
                   {
                      "propertyId": "executorForcePullImage",
                      "propertyName": "SETTINGS._SPARK_DOCKER_FORCE_PULL_IMAGE_",
                      "propertyType": "boolean",
                      "default": false,
                      "width": "auto",
                      "newRow": true,
                      "tooltip": "Force pull image for each execution",
                      "qa": "workflow-spark-executorForcePullImage"
                   }
                ]
             },
            {
                "name": "sparkMesosConf",
                "listCompressed": true,
                "properties": [
                   {
                      "propertyId": "mesosNativeJavaLibrary",
                      "width": 4,
                      "propertyName": "SETTINGS._SPARK_MESOS_NATIVE_LIBRARY_",
                      "propertyType": "text",
                      "default": "/opt/mesosphere/lib/libmesos.so",
                      "tooltip": "Mesos native library location",
                      "qa": "workflow-spark-mesosNativeJavaLibrary"
                   },
                   {
                      "propertyId": "mesosHDFSConfURI",
                      "width": 4,
                      "propertyName": "SETTINGS._SPARK_MESOS_HDFS_CONF_URI_",
                      "propertyType": "text",
                      "tooltip": "Download HDFS configuration in Stratio's Spark fork",
                      "qa": "workflow-spark-mesos-HDFS-ConfURI"
                   }
                ]
             },
            {
              "propertyId": "userSparkConf",
              "propertyName": "SETTINGS._SPARK_CONF_",
              "propertyType": "list",
              "required": false,
              "tooltip": "Spark configuration properties. Values set here will override the ones set in the configuration file.",
              "qa": "workflow-spark-conf",
              "fields": [
                 {
                   "propertyId": "sparkConfKey",
                   "propertyName": "SETTINGS._SPARK_CONF_KEY_",
                   "position": "left",
                   "propertyType": "text",
                   "regexp": "",
                   "default": "",
                    "width": 4,
                   "required": false,
                   "tooltip": "Spark configuration property with the structure spark.xxx",
                   "qa": "workflow-sparkConfKey"
                 },
                 {
                   "propertyId": "sparkConfValue",
                   "propertyName": "SETTINGS._SPARK_CONF_VALUE_",
                   "position": "right",
                   "propertyType": "text",
                   "regexp": "",
                   "default": "",
                    "width": 4,
                   "required": false,
                   "qa": "workflow-sparkConfValue"
                 }
              ]
            },
             {
                "propertyId": "executorExtraJavaOptions",
                "propertyName": "SETTINGS._SPARK_EXECUTOR_JAVA_OPTIONS_",
                "propertyType": "text",
                "width": 4,
                "default": "-XX:+UseConcMarkSweepGC",
                "tooltip": "Extra Java options to pass to the executors",
                "qa": "workflow-spark-executorJavaOptions"
             },
             {
                "propertyId": "sparkLocalDir",
                "propertyName": "SETTINGS._SPARK_LOCAL_DIR_",
                "propertyType": "text",
                "width": 4,
                "default": "/opt/spark/dist",
                "tooltip": "Set the spark home in Mesos deployments",
                "qa": "workflow-spark-home"
             },
             {
                "propertyId": "sparkUser",
                "propertyName": "SETTINGS._SPARK_USER_",
                "propertyType": "text",
                "width": 4,
                "tooltip": "Sets the spark user in Mesos deployments",
                "qa": "workflow-spark-user"
             },
            {
                "propertyId": "coarse",
                "propertyName": "SETTINGS._SPARK_COARSE_",
                "propertyType": "boolean",
                "newRow": true,
                "required": true,
                "width": "auto",
                "default": true,
                "tooltip": "Coarse mode is recommended in Streaming Applications over Mesos Clusters. If assigned to false is used the Spark fine grained mode",
                "qa": "workflow-spark-coarse"
             },
             {
                "propertyId": "parquetBinaryAsString",
                "propertyName": "SETTINGS._SPARK_PARQUET_BINARY_STRING_",
                "propertyType": "boolean",
                "required": true,
                "width": "auto",
                "default": true,
                "tooltip": "Tells Spark SQL to treat binary-encoded data as strings",
                "qa": "workflow-spark-parquetBinaryAsString"
             },
            {
                "propertyId": "stopGracefully",
                "propertyName": "SETTINGS._SPARK_STOP_GRACEFULLY_",
                "propertyType": "switch",
                "required": true,
                "width": "auto",
                "default": true,
                "tooltip": "Stop gracefully the streaming contexts when shutting down the spark applications",
                "qa": "workflow-spark-stopGracefully"
             },
            {
              "propertyId": "stopGracefulTimeout",
              "propertyName": "SETTINGS._SPARK_STOP_GRACEFUL_TIMEOUT_",
              "propertyType": "text",
              "width": 4,
               "visible": [
                  [
                     {
                        "propertyId": "stopGracefully",
                        "value": true
                     }
                  ]
               ],
              "tooltip": "Turn this down to prevent long blocking at shutdown",
              "qa": "workflow-spark-stopGracefulTimeout"
            }
          ]
        }
      ]
    }
  ],
  "helpLinks": [
  ]
}
