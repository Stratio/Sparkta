[
    {
        "name": "Tokenizer",
        "className": "org.apache.spark.ml.feature.Tokenizer",
        "uid": "t1",
        "properties": {
            "inputCol": "text",
            "outputCol": "words"
        }
    },
    {
        "name": "HashingTF",
        "className": "org.apache.spark.ml.feature.HashingTF",
        "uid": "h1",
        "properties": {
            "inputCol": "words",
            "numFeatures": "1000",
            "outputCol": "features",
            "alpha_invented": "aaaa"
        }
    },
    {
        "name": "LogisticRegression",
        "uid": "l1",
        "className": "org.apache.spark.ml.classification.LogisticRegression",
        "properties": {
            "maxIter": "10",
            "regParam": "0.001",
            "fitIntercept": false
        }
    }
]