MongoDB Specifications
******************

- :ref:`introduction-label`

- :ref:`driver-label`

- :ref:`worker-label`


.. _introduction-label:

Introduction
============

This output uses the native driver for MongoDB in Scala language. **Casbah Driver** offers all the functionality that
Sparkta need to insert,update and add and configure the database for proper system operation.

In the implementation of an Output of Sparkta's SDK there are two possibilities: one could be to allow transformations
 over data to a Spark's DataFrame. Other posibility could be to insert directly a **UpdateMetricOperation**. This
 output does not use any kind of Spark plugin to insert DataFrames.

This plugin creates one client connection per Worker in a Spark Cluster.

Is necessary need to override two functions from the Output SDK:
::
  override def doPersist(stream: DStream[(DimensionValuesTime, Map[String, Option[Any]])]): Unit
  override def upsert(metricOperations: Iterator[(DimensionValuesTime, Map[String, Option[Any]])]): Unit


.. _driver-label:

Driver Operations
============

When the driver starts the Output, several processes for creating indexes needed to run the various collections.
There are two types of indexes:

  * Unique index:

    Create primary key that contains a field called "id" values separated by "_" and the field timeBucket buckets.
    This index is optional, because if you do not specify timeBucket not created and the primary key of the
    collection is "_id" field

  * Text index:

    If we do a text index on any bucket or some aggregate field, the system will create it for us.

      - Example:
      ::

        [
          {
            "v" : 1,
            "key" : {
              "_id" : 1
            },
            "name" : "_id_",
            "ns" : "sparkta.precision3_hastags_retweets_minute"
          },
          {
            "v" : 1,
            "key" : {
              "_fts" : "text",
              "_ftsx" : 1
            },
            "name" : "userLocation",
            "ns" : "sparkta.precision3_hastags_retweets_minute",
            "background" : true,
            "default_language" : "english",
            "weights" : {
              "userLocation" : 1
            },
            "language_override" : "language",
            "textIndexVersion" : 2
          },
          {
            "v" : 1,
            "unique" : true,
            "key" : {
              "id" : 1,
              "minute" : 1
            },
            "name" : "id_minute",
            "ns" : "sparkta.precision3_hastags_retweets_minute",
            "background" : true
          }
        ]


.. _worker-label:

Worker Operations
============

As this Output does not use functionality of DataFrames, override the method Upsert, that save all values
of a **Tuple -> (DimensionValuesTime, Aggregations)**.
Below you can see each of the features implemented:

  * Each Worker save in one BulkOperation for each data partition of a RDD.

  * The output create one collection for each cube. With the name "bucket1_bucket2..." + timeBucket is
    specified in properties for the stateful operations)

      - Example: (with multiplexer)
      ::

          hastags_minute
          hastags_retweets_minute
          hastags_retweets_urls_minute
          hastags_urls_minute
          precision3_hastags_minute
          precision3_hastags_retweets_minute
          precision3_hastags_retweets_urls_minute
          precision3_hastags_urls_minute
          precision3_minute
          precision3_retweets_minute
          precision3_retweets_urls_minute
          precision3_urls_minute
          retweets_minute
          retweets_urls_minute
          system.indexes
          urls_minute


  * The output upsert documents with the _id field "bucket1_bucket2...". If timeBucket
    is specified in properties the system save the data in two fields "id" with the buckets values and timeBucket
    field with the dateTime of the document. With the second the _id is autogenerated.

      - Example:
      ::

          "_id" : ObjectId("554891b3da00bdd0c284a573"),
          "id" : "List(0.703125, 0.703125)_1_0",
          "minute" : ISODate("2015-05-05T09:47:00Z"),
          "min_wordsN" : 1,
          "stddev_wordsN" : 2.8284271247461903,
          "avg_wordsN" : 6,
          "language" : "english",
          "variance_wordsN" : 8,
          "last_retweets" : NumberLong(0),
          "median_wordsN" : 6,
          "count" : NumberLong(750),
          "sum_wordsN" : NumberLong(7669),
          "max_wordsN" : 29,


  * MongoDB have several **Update Aggregation Commands** that are used by Sparkta for insert the aggregate fields. As
   can be Sum, Count, Avg, Max, Min ...
